# Yelp Fake Review Detection
# Sophia Tsilerides (smt570), Ilana Weinstein (igw212, submiter), Amanda Kuznecov (anr431)
# DS-GA 1003 - New York University 

import random
random.seed(2020)

import pandas as pd
import numpy as np
import matplotlib as mpl
import matplotlib.pyplot as plt
import seaborn as sns
mpl.rcParams['figure.figsize'] = (12, 10)
colors = plt.rcParams['axes.prop_cycle'].by_key()['color']

import time
import os
import tempfile

from tqdm import tqdm
from gensim.models.doc2vec import Doc2Vec, TaggedDocument
from nltk.tokenize import word_tokenize
from gensim.test.utils import common_texts, get_tmpfile

import tensorflow as tf
from tensorflow import keras

from scipy import sparse

import sklearn
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import average_precision_score
from sklearn.metrics import roc_curve, auc

import utilities

import warnings
warnings.filterwarnings('ignore')

############ Preprocessing 
train = pd.DataFrame(pd.read_csv(os.getcwd() + '/' + 'test.csv', sep=','))
dev = pd.DataFrame(pd.read_csv(os.getcwd() + '/' + 'dev.csv', sep=','))
test = pd.DataFrame(pd.read_csv(os.getcwd() + '/' + 'test_no_label.csv', sep=','))


train = utilities.addDomainFeatures(train)
dev = utilities.addDomainFeatures(dev)
test = utilities.addDomainFeatures(test)

utilities.preprocessing(train)
utilities.preprocessing(dev)
utilities.preprocessing(test)

train = utilities.POSTagging(train)
dev = utilities.POSTagging(dev)
test = utilities.POSTagging(test)

# Save locally
train.to_csv(os.getcwd() + '/' + 'train_clean.csv', index = False, header=True)
dev.to_csv(os.getcwd() + '/' + 'dev_clean.csv', index = False, header=True)
test.to_csv(os.getcwd() + '/' + 'test_clean.csv', index = False, header=True)

############ Vectorizing
train_clean = pd.DataFrame(pd.read_csv(os.getcwd() + '/' + 'train_clean.csv', sep=','))
dev_clean = pd.DataFrame(pd.read_csv(os.getcwd() + '/' + 'dev_clean.csv', sep=','))
test_clean = pd.DataFrame(pd.read_csv(os.getcwd() + '/' + 'test_clean.csv', sep=','))

train = utilities.tokenize(train_clean)
dev = utilities.tokenize(dev_clean)
test = utilities.tokenize(test_clean)

train_doc = utilities.Doc2Vec_input(train)
dev_doc = utilities.Doc2Vec_input(dev)
test_doc = utilities.Doc2Vec_input(test)

# Load saved model
dv_model = Doc2Vec.load('dv_model')

# Create feature vectors for test data using trained model
dv_train = utilities.Doc2Vec_vectorize(dv_model, train_doc)
dv_dev = utilities.Doc2Vec_vectorize(dv_model, dev_doc)
dv_test = utilities.Doc2Vec_vectorize(dv_model, test_doc)

# Save Vectorizer
np.savetxt("dv_train.csv", dv_train, delimiter=",")
np.savetxt("dv_dev.csv", dv_dev, delimiter=",")
np.savetxt("dv_test.csv", dv_test, delimiter=",")

############ Modeling

# Load Preprocessed data and Doc2Vec vecotorizer and concatenate 
full_preprocessed_train = pd.DataFrame(pd.read_csv(os.getcwd() + '/' + 'train_clean.csv', sep=','))
full_preprocessed_val = pd.DataFrame(pd.read_csv(os.getcwd() + '/' + 'dev_clean.csv', sep=','))
test =  pd.DataFrame(pd.read_csv(os.getcwd() + '/' + 'test_clean.csv', sep=','))

full_preprocessed_train = full_preprocessed_train.replace(np.nan, " ")
full_preprocessed_val = full_preprocessed_val.replace(np.nan, " ")
test = test.replace(np.nan, " ")

doc2vec_train = pd.DataFrame(pd.read_csv(os.getcwd() + '/' + 'dv_train.csv', sep=',', header=None))
doc2vec_val = pd.DataFrame(pd.read_csv(os.getcwd() + '/' + 'dv_val.csv', sep=',', header=None))
dv_test = pd.DataFrame(pd.read_csv(os.getcwd() + '/' + 'dv_test.csv', sep=',', header=None))

doc2vec_X_training_data = pd.concat([full_preprocessed_train, doc2vec_train], axis=1, sort=False)
doc2vec_X_val_data = pd.concat([full_preprocessed_val, doc2vec_val], axis=1, sort=False)
test = pd.concat([test, dv_test], axis=1, sort=False)

# Examine class label imbalance
neg, pos = np.bincount(doc2vec_X_training_data['label'])
total = neg + pos
print('Examples:\n    Total: {}\n    Positive: {} ({:.2f}% of total)\n'.format(
    total, pos, 100 * pos / total))

# Clean and Normalize the data
cleaned_train = doc2vec_X_training_data.copy()
cleaned_val = doc2vec_X_val_data.copy()
cleaned_test = test.copy()

# You don't want the `Time` column.
cleaned_train = cleaned_train.drop(['date'], axis=1)
cleaned_val = cleaned_val.drop(['date'], axis=1)
cleaned_test = cleaned_test.drop(['date'], axis=1)

# The `length` column covers a huge range. Convert to log-space.
eps=0.001 
cleaned_train['length'] = np.log(cleaned_train.pop('length')+eps)
cleaned_val['length'] = np.log(cleaned_val.pop('length')+eps)
cleaned_test['length'] = np.log(cleaned_test.pop('length')+eps)

# Can't normalize str col
cleaned_train = cleaned_train.drop(['review'], axis = 1)
cleaned_val = cleaned_val.drop(['review'], axis = 1)
cleaned_test = cleaned_test.drop(['review'], axis = 1)

cleaned_test = cleaned_test.drop(['label'], axis = 1)

# Form np arrays of labels and features.
train_df, test_df = cleaned_train, cleaned_test
val_df = cleaned_val

train_labels = np.array(train_df.pop('label'))
val_labels = np.array(val_df.pop('label'))

train_features = np.array(train_df)
val_features = np.array(val_df)
test_features = np.array(test_df)

scaler = StandardScaler()
train_features = scaler.fit_transform(train_features)

val_features = scaler.transform(val_features)
test_features = scaler.transform(test_features)

train_features = np.clip(train_features, -5, 5)
val_features = np.clip(val_features, -5, 5)
test_features = np.clip(test_features, -5, 5)


print('Training labels shape:', train_labels.shape)
print('Validation labels shape:', val_labels.shape)

print('Training features shape:', train_features.shape)
print('Validation features shape:', val_features.shape)
print('Test features shape:', test_features.shape)

# Define Model and metrics
METRICS = [
      keras.metrics.TruePositives(name='tp'),
      keras.metrics.FalsePositives(name='fp'),
      keras.metrics.TrueNegatives(name='tn'),
      keras.metrics.FalseNegatives(name='fn'), 
      keras.metrics.BinaryAccuracy(name='accuracy'),
      keras.metrics.Precision(name='precision'),
      keras.metrics.Recall(name='recall'),
      keras.metrics.AUC(name='auc'),
]

def make_model(metrics = METRICS, output_bias=None):
    if output_bias is not None:
        output_bias = tf.keras.initializers.Constant(output_bias)
    # Sequential groups a linear stack of layers into a tf.keras.Model.
    model = keras.Sequential([
        keras.layers.Dense(64, activation='relu', input_shape=(train_features.shape[-1],)),
        keras.layers.Dropout(0.5),
#         keras.layers.Dense(64, activation='relu'),
#         keras.layers.Dropout(0.5),
        keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias),])

    model.compile(
      optimizer=keras.optimizers.Adam(lr=.1),
      loss=keras.losses.BinaryCrossentropy(),
      metrics=metrics)

    return model

EPOCHS = 100
BATCH_SIZE = 10000

early_stopping = tf.keras.callbacks.EarlyStopping(
    monitor='val_auc', 
    verbose=1,
    patience=10,
    mode='max',
    restore_best_weights=True)

# Checkpoint the initial weights
initial_bias = np.log([pos/neg])
model = make_model(output_bias = initial_bias)
initial_weights = os.path.join(tempfile.mkdtemp(),'initial_weights')
model.save_weights(initial_weights)

# Calculate class weights
# Scaling by total/2 helps keep the loss to a similar magnitude.
# The sum of the weights of all examples stays the same.
weight_for_0 = (1 / neg)*(total)/2.0 
weight_for_1 = (1 / pos)*(total)/2.0

class_weight = {0: weight_for_0, 1: weight_for_1}

# Train model with class weights
weighted_model = make_model()
weighted_model.load_weights(initial_weights)

weighted_history = weighted_model.fit(
    train_features,
    train_labels,
    batch_size=BATCH_SIZE,
    epochs=EPOCHS,
    callbacks = [early_stopping],
    validation_data=(val_features, val_labels),
    # The class weights go here
    class_weight=class_weight) 

# Evaluate 
test_predictions_weighted = weighted_model.predict(test_features, batch_size=BATCH_SIZE)

# Save predicitions
test_predictions_weighted = pd.DataFrame(test_predictions_weighted)
test_predictions_weighted.to_csv (os.getcwd() + '/' + 'predictions.csv', index = False, header=False)